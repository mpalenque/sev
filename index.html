<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Skin Smoothing with MediaPipe ImageSegmenter</title>
<style>
  body {
    font-family: Arial, sans-serif;
    margin: 20px;
    display: flex;
    flex-direction: column;
    align-items: center;
    background-color: #f0f0f0;
    background-image: url('sev.png');
    background-size: cover; /* Optional: to make the image cover the entire background */
    background-repeat: no-repeat; /* Optional: to prevent the image from repeating */
    background-position: center center; /* Optional: to center the image */
  }
  #liveViewContainer {
    position: relative;
    border: 1px solid #ccc;
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    background-color: transparent; /* Ensure this is transparent */
    padding: 10px;
    border-radius: 8px;
  }
  video {
    display: none; /* Hide video element, we'll draw to canvas */
  }
  canvas {
    display: block;
    border: 1px solid #ddd;
  }
  .controls {
    margin-top: 15px;
    padding: 15px;
    background-color: #fff;
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    display: flex;
    flex-direction: column;
    align-items: center;
  }
  .controls button {
    padding: 10px 15px;
    font-size: 16px;
    cursor: pointer;
    border: none;
    border-radius: 5px;
    background-color: #007bff;
    color: white;
    margin-bottom: 10px;
  }
  .controls button:hover {
    background-color: #0056b3;
  }
  .controls label {
    margin-bottom: 5px;
  }
  .controls input[type="range"] {
    width: 200px;
  }
  #loadingMessage {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 1.2em;
    color: #333;
    background-color: rgba(255, 255, 255, 0.8);
    padding: 10px;
    border-radius: 5px;
    display: none; /* Hidden by default */
  }
</style>
</head>
<body>
  

  <div id="liveViewContainer">
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output_canvas" width="640" height="480"></canvas>
    <div id="loadingMessage">Loading model and webcam...</div>
  </div>

  <div class="controls">
    <button id="webcamButton">ENABLE WEBCAM</button>
    <button id="takePhotoButton" style="margin-top: 10px;">TAKE PHOTO</button>
    <label for="smoothnessSlider">Smoothness:</label>
    <input type="range" id="smoothnessSlider" min="0" max="20" value="5">
  </div>

  <script type="module">
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3"; // Changed import style
    const { ImageSegmenter, FaceLandmarker, FilesetResolver } = vision; // Destructure necessary components

    let imageSegmenter;
    let faceLandmarker;
    let runningMode = "VIDEO";
    const video = document.getElementById("webcam");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d", { alpha: true }); // Removed willReadFrequently
    const webcamButton = document.getElementById("webcamButton");
    const smoothnessSlider = document.getElementById("smoothnessSlider");
    const loadingMessage = document.getElementById("loadingMessage");
    const takePhotoButton = document.getElementById("takePhotoButton"); // New button reference

    const blurScaleFactor = 0.5; // Process blur on an image 50% of original size
    const landmarkMaskScaleFactor = 0.5; // New: Scale factor for landmark masks
    const landmarkMaskBlurRadius = 3; // New: Blur radius for landmark masks (applied to scaled-down mask)
    const SLIDER_POWER_CURVE = 3; // Power for non-linear slider response (e.g., 2 for quadratic)

    // Function to calculate effective blur amount based on slider's raw value
    function calculateEffectiveBlur(rawValue, sliderMax, power) {
      if (sliderMax === 0) return 0; // Avoid division by zero
      const normalizedValue = rawValue / sliderMax;
      return Math.pow(normalizedValue, power) * sliderMax;
    }

    let blurAmount = calculateEffectiveBlur(
      parseFloat(smoothnessSlider.value),
      parseFloat(smoothnessSlider.max),
      SLIDER_POWER_CURVE
    );

    smoothnessSlider.addEventListener("input", (event) => {
      blurAmount = calculateEffectiveBlur(
        parseFloat(event.target.value),
        parseFloat(smoothnessSlider.max),
        SLIDER_POWER_CURVE
      );
    });

    // Offscreen canvases
    const blurredVideoCanvas = document.createElement("canvas");
    const blurredVideoCtx = blurredVideoCanvas.getContext("2d", { alpha: true }); // Removed willReadFrequently
    const maskCanvas = document.createElement("canvas");
    const maskCanvasCtx = maskCanvas.getContext("2d", { alpha: true }); // Removed willReadFrequently
    const effectLayerCanvas = document.createElement("canvas");
    const effectLayerCtx = effectLayerCanvas.getContext("2d", { alpha: true }); // Removed willReadFrequently

    // New canvas for eye mask
    const eyeMaskCanvas = document.createElement("canvas");
    const eyeMaskCtx = eyeMaskCanvas.getContext("2d", { alpha: true }); // Removed willReadFrequently

    // New canvas for the face oval mask
    const faceOvalMaskCanvas = document.createElement("canvas");
    const faceOvalMaskCtx = faceOvalMaskCanvas.getContext("2d", { alpha: true }); // Removed willReadFrequently

    // New: Declare tempBlurLayerCanvas and tempBlurLayerCtx globally
    let tempBlurLayerCanvas;
    let tempBlurLayerCtx;

    // New: Canvases for person segmentation (background removal)
    const personMaskCanvas = document.createElement("canvas");
    const personMaskCtx = personMaskCanvas.getContext("2d");
    const personOnlyCanvas = document.createElement("canvas");
    const personOnlyCtx = personOnlyCanvas.getContext("2d");

    // For selfie_multiclass_256x256 model
    // Categories: 0:background, 1:hair, 2:body-skin, 3:face-skin, 4:clothes, 5:others
    const skinCategoryIndices = [3]; // REVERTED: Target only face-skin for smoothing
    const personCategoryIndices = [1, 2, 3, 4]; // Hair, body-skin, face-skin, clothes for background removal

    async function createMediaPipeTasks() {
      loadingMessage.textContent = 'Initializing MediaPipe tasks...';
      loadingMessage.style.display = 'block';
      console.log("Attempting to create MediaPipe tasks...");
      let visionFilesetResolver;
      try {
        visionFilesetResolver = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
      } catch (e) {
        console.error("Failed to load FilesetResolver for Vision Tasks:", e);
        loadingMessage.textContent = 'Error: Could not load MediaPipe vision tasks. Check console.';
        return;
      }
      console.log("FilesetResolver loaded.");

      // Create ImageSegmenter
      let imageSegmenterDelegate = "GPU";
      try {
        loadingMessage.textContent = 'Loading Image Segmenter model (GPU)...';
        const segmenterStartTime = performance.now();
        imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
            delegate: "GPU"
          },
          runningMode: runningMode,
          outputCategoryMask: true,
          outputConfidenceMasks: false
        });
        const segmenterLoadTime = performance.now() - segmenterStartTime;
        console.log(`ImageSegmenter (GPU) created successfully in ${segmenterLoadTime.toFixed(2)}ms.`);
      } catch (gpuError) {
        console.warn("Failed to create ImageSegmenter with GPU delegate:", gpuError);
        imageSegmenterDelegate = "CPU";
        loadingMessage.textContent = 'GPU failed for Image Segmenter. Trying CPU...';
        try {
          const segmenterStartTime = performance.now();
          imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
            baseOptions: {
              modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
              delegate: "CPU"
            },
            runningMode: runningMode,
            outputCategoryMask: true,
            outputConfidenceMasks: false
          });
          const segmenterLoadTime = performance.now() - segmenterStartTime;
          console.log(`ImageSegmenter (CPU) created successfully in ${segmenterLoadTime.toFixed(2)}ms after GPU fallback.`);
        } catch (cpuError) {
          console.error("Failed to create ImageSegmenter with CPU delegate after GPU failure:", cpuError);
          loadingMessage.textContent = 'Error: Could not load Image Segmenter (GPU & CPU). Check console.';
          return;
        }
      }

      // Create FaceLandmarker
      let faceLandmarkerDelegate = "GPU";
      try {
        loadingMessage.textContent = `Loading Face Landmarker model (GPU)... (Image Segmenter: ${imageSegmenterDelegate})`;
        const landmarkerStartTime = performance.now();
        faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
          },
          outputFaceBlendshapes: false,
          outputFacialTransformationMatrixes: false,
          runningMode: runningMode,
          numFaces: 1
        });
        const landmarkerLoadTime = performance.now() - landmarkerStartTime;
        console.log(`FaceLandmarker (GPU) created successfully in ${landmarkerLoadTime.toFixed(2)}ms.`);
      } catch (gpuError) {
        console.warn("Failed to create FaceLandmarker with GPU delegate:", gpuError);
        faceLandmarkerDelegate = "CPU";
        loadingMessage.textContent = `GPU failed for Face Landmarker. Trying CPU... (Image Segmenter: ${imageSegmenterDelegate})`;
        try {
          const landmarkerStartTime = performance.now();
          faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
            baseOptions: {
              modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
              delegate: "CPU"
            },
            outputFaceBlendshapes: false,
            outputFacialTransformationMatrixes: false,
            runningMode: runningMode,
            numFaces: 1
          });
          const landmarkerLoadTime = performance.now() - landmarkerStartTime;
          console.log(`FaceLandmarker (CPU) created successfully in ${landmarkerLoadTime.toFixed(2)}ms after GPU fallback.`);
        } catch (cpuError) {
          console.error("Failed to create FaceLandmarker with CPU delegate after GPU failure:", cpuError);
          loadingMessage.textContent = `Error: Could not load Face Landmarker (GPU & CPU). Image Segmenter: ${imageSegmenterDelegate}. Check console.`;
          return;
        }
      }
      
      loadingMessage.textContent = `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`;
      setTimeout(() => {
          if (loadingMessage.textContent === `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`) {
            loadingMessage.style.display = 'none';
          }
      }, 1500); // Increased timeout slightly
    }

    async function enableCam() {
      if (!imageSegmenter || !faceLandmarker) {
        loadingMessage.textContent = 'Models not yet loaded. Please wait...';
        loadingMessage.style.display = 'block';
        await createMediaPipeTasks();
        if (!imageSegmenter || !faceLandmarker) {
            console.error("MediaPipe tasks failed to initialize. Cannot enable webcam features.");
            webcamButton.disabled = false;
            return;
        }
      }

      if (webcamButton.textContent === "ENABLE WEBCAM") {
        webcamButton.disabled = true;
        loadingMessage.textContent = 'Accessing webcam...';
        loadingMessage.style.display = 'block';
        const constraints = { video: true };
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          video.srcObject = stream;
          
          const startPredictionLogic = () => {
            console.log(`Video 'playing' event fired. Raw dimensions: ${video.videoWidth}x${video.videoHeight}`);
            // Remove the event listener after it has run once
            video.removeEventListener("playing", startPredictionLogic);

            if (video.videoWidth === 0 || video.videoHeight === 0) {
                console.error("CRITICAL: Video dimensions are zero at 'playing' event. Effect will likely fail.");
                loadingMessage.textContent = 'Error: Video stream has zero dimensions.';
                loadingMessage.style.display = 'block';
                webcamButton.disabled = false;
                isPredicting = false; 
                return;
            }
            
            canvasElement.width = video.videoWidth;
            canvasElement.height = video.videoHeight;
            
            // Set blurredVideoCanvas to a scaled down size
            blurredVideoCanvas.width = video.videoWidth * blurScaleFactor;
            blurredVideoCanvas.height = video.videoHeight * blurScaleFactor;
            
            effectLayerCanvas.width = video.videoWidth;
            effectLayerCanvas.height = video.videoHeight;
            eyeMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; // Modified
            eyeMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; // Modified
            faceOvalMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; // Modified
            faceOvalMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; // Modified
            // maskCanvas and personMaskCanvas will be set by drawMaskFromPixelData based on segmentation output size

            // New: Initialize personOnlyCanvas size
            personOnlyCanvas.width = video.videoWidth;
            personOnlyCanvas.height = video.videoHeight;

            // New: Initialize tempBlurLayerCanvas and tempBlurLayerCtx
            if (!tempBlurLayerCanvas) {
                tempBlurLayerCanvas = document.createElement('canvas');
                tempBlurLayerCtx = tempBlurLayerCanvas.getContext('2d');
            }
            tempBlurLayerCanvas.width = video.videoWidth;
            tempBlurLayerCanvas.height = video.videoHeight;

            loadingMessage.style.display = 'none';
            webcamButton.textContent = "DISABLE WEBCAM";
            webcamButton.disabled = false;
            
            console.log("[startPredictionLogic] Setting isPredicting = true.");
            isPredicting = true; 
            console.log("[startPredictionLogic] Calling predictWebcam for the first time.");
            predictWebcam();
          };

          video.addEventListener("playing", startPredictionLogic);
          
          video.play().then(() => {
            console.log("Video play() promise resolved. Playback should be starting or started.");
          }).catch(e => {
            console.error("Video play failed:", e);
            loadingMessage.textContent = 'Error playing video. Check permissions.';
            webcamButton.disabled = false;
            video.removeEventListener("playing", startPredictionLogic); // Clean up listener on error
          });

        } catch (err) {
          console.error("Error accessing webcam: ", err);
          loadingMessage.textContent = 'Error accessing webcam. Please allow access and try again.';
          webcamButton.disabled = false;
        }
      } else {
        video.srcObject.getTracks().forEach(track => track.stop());
        video.srcObject = null;
        webcamButton.textContent = "ENABLE WEBCAM";
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        isPredicting = false;
      }
    }
    webcamButton.addEventListener("click", enableCam);

    // Helper function to convert Data URL to Blob
    async function dataURLtoBlob(dataurl) {
      const res = await fetch(dataurl);
      return await res.blob();
    }

    // Handle Take Photo button click
    async function handleTakePhotoClick() {
      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        alert("Please enable the webcam and ensure it's active before taking a photo.");
        return;
      }

      try {
        const dataUrl = canvasElement.toDataURL("image/png");
        const blob = await dataURLtoBlob(dataUrl);
        const fileName = "photo.png";
        const file = new File([blob], fileName, { type: "image/png" });
        
        const shareData = {
          files: [file],
          title: "My Photo",
          text: "Check out this photo I took!",
        };

        if (navigator.share && navigator.canShare && navigator.canShare(shareData)) {
          await navigator.share(shareData);
          console.log("Photo shared successfully or share dialog opened.");
        } else {
          console.warn("Web Share API not supported or cannot share this data. Falling back to download.");
          // Fallback: Download the image
          const link = document.createElement("a");
          link.href = dataUrl;
          link.download = fileName;
          document.body.appendChild(link);
          link.click();
          document.body.removeChild(link);
          alert("Photo downloaded as Web Share is not available.");
        }
      } catch (error) {
        console.error("Error taking or sharing photo:", error);
        alert(`Error: ${error.message}`);
        // Fallback for error during share (e.g., user cancelled)
        if (error.name !== 'AbortError') { // AbortError means user cancelled share dialog
            const dataUrl = canvasElement.toDataURL("image/png");
            const link = document.createElement("a");
            link.href = dataUrl;
            link.download = "photo_fallback.png";
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            alert("Sharing failed. Photo downloaded instead.");
        }
      }
    }

    takePhotoButton.addEventListener("click", handleTakePhotoClick);

    function drawMaskFromPixelData(maskPixelData, maskWidth, maskHeight, targetCtx, targetIndices) {
      if (!maskPixelData || maskPixelData.length === 0) {
        console.error("Invalid or empty maskPixelData received in drawMaskFromPixelData");
        targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
        return;
      }
      
      if (targetCtx.canvas.width !== maskWidth || targetCtx.canvas.height !== maskHeight) {
          targetCtx.canvas.width = maskWidth;
          targetCtx.canvas.height = maskHeight;
      }

      const newImageData = targetCtx.createImageData(maskWidth, maskHeight);
      const newPixelData = newImageData.data; 

      if (maskPixelData.length !== maskWidth * maskHeight) {
          console.error(`maskPixelData.length (${maskPixelData.length}) does not match maskWidth*maskHeight (${maskWidth*maskHeight}).`);
          targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
          return;
      }

      for (let i = 0; i < maskPixelData.length; i++) {
        const category = maskPixelData[i];
        const offset = i * 4;
        if (targetIndices.includes(category)) {
          newPixelData[offset] = 0;     // R (can be any color, alpha is key)
          newPixelData[offset + 1] = 0; // G
          newPixelData[offset + 2] = 0; // B
          newPixelData[offset + 3] = 255; // Alpha: Opaque for skin
        } else {
          newPixelData[offset + 3] = 0;   // Alpha: Transparent for non-skin
        }
      }
      targetCtx.putImageData(newImageData, 0, 0);
    }

    let lastVideoTime = -1;
    let isPredicting = false;

    async function predictWebcam() {
      console.log(`[predictWebcam START] isPredicting: ${isPredicting}, video.paused: ${video.paused}, video.ended: ${video.ended}, video.readyState: ${video.readyState}, blurAmount: ${blurAmount}`);

      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        isPredicting = false;
        console.log("[predictWebcam] Loop stopping: initial check failed (not predicting, no src, paused, or ended).");
        return;
      }

      if (video.readyState < video.HAVE_ENOUGH_DATA || !imageSegmenter || !faceLandmarker) {
          console.log(`[predictWebcam] Video not ready (readyState ${video.readyState}) or models not loaded. Retrying next frame.`);
          if (isPredicting) requestAnimationFrame(predictWebcam);
          return;
      }

      let startTimeMs = performance.now();
      let segmentationResultsToClose = null; // To ensure close is called

      if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        
        const segmentationResults = imageSegmenter.segmentForVideo(video, startTimeMs);
        if (segmentationResults && segmentationResults.categoryMask) {
            segmentationResultsToClose = segmentationResults.categoryMask;
        }
        console.log("Segmentation results:", segmentationResults);
        const landmarkResults = faceLandmarker.detectForVideo(video, startTimeMs);
        console.log("Landmark results:", landmarkResults);

        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        // canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height); // Original drawing, will be replaced

        // --- Background Removal Step ---
        let personDrawnSuccessfully = false;
        if (segmentationResults && segmentationResults.categoryMask) {
          const fullMask = segmentationResults.categoryMask;
          const fullMaskPixelData = fullMask.getAsUint8Array();
          const fullMaskWidth = fullMask.width;
          const fullMaskHeight = fullMask.height;

          if (fullMaskPixelData && fullMaskWidth > 0 && fullMaskHeight > 0) {
            // Ensure personMaskCanvas is sized correctly
            if (personMaskCanvas.width !== fullMaskWidth || personMaskCanvas.height !== fullMaskHeight) {
              personMaskCanvas.width = fullMaskWidth;
              personMaskCanvas.height = fullMaskHeight;
            }
            // Create a mask for the entire person
            drawMaskFromPixelData(fullMaskPixelData, fullMaskWidth, fullMaskHeight, personMaskCtx, personCategoryIndices);

            // Draw video to personOnlyCanvas
            personOnlyCtx.clearRect(0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.drawImage(video, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            // Apply person mask to make background transparent
            personOnlyCtx.globalCompositeOperation = 'destination-in';
            personOnlyCtx.drawImage(personMaskCanvas, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.globalCompositeOperation = 'source-over'; // Reset composite operation

            // Draw the person (with transparent background) to the main canvas
            canvasCtx.drawImage(personOnlyCanvas, 0, 0, canvasElement.width, canvasElement.height);
            personDrawnSuccessfully = true;
          }
        }

        if (!personDrawnSuccessfully) {
          // Fallback: if person segmentation failed, draw the original video
          console.warn("[predictWebcam] Person segmentation failed or no mask. Drawing raw video.");
          canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);
        }
        // --- End Background Removal Step ---

        // --- Skin Smoothing Step (Applied on top of whatever is on canvasCtx) ---
        if (segmentationResults && segmentationResults.categoryMask && landmarkResults && landmarkResults.faceLandmarks && landmarkResults.faceLandmarks.length > 0) {
          console.log("CategoryMask available for smoothing:", !!segmentationResults.categoryMask);
          console.log("FaceLandmarks available for smoothing:", landmarkResults.faceLandmarks.length);
          // Note: skinMaskPixelData, skinMaskWidth, skinMaskHeight are already available from the earlier part of this block
          // if we assume segmentationResults.categoryMask is the same one.
          // For clarity, let's re-access it, though it's the same data.
          const skinMaskForSmoothing = segmentationResults.categoryMask;
          const skinMaskPixelData = skinMaskForSmoothing.getAsUint8Array();
          const skinMaskWidth = skinMaskForSmoothing.width;
          const skinMaskHeight = skinMaskForSmoothing.height;
          const landmarks = landmarkResults.faceLandmarks[0];
          console.log(`Skin mask dimensions for smoothing: ${skinMaskWidth}x${skinMaskHeight}. Pixel data length: ${skinMaskPixelData ? skinMaskPixelData.length : 'null'}`);

          if (skinMaskPixelData && skinMaskWidth > 0 && skinMaskHeight > 0) {
            blurredVideoCtx.clearRect(0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);
            // Draw the *original video* frame downscaled onto the smaller blurredVideoCanvas for blurring
            // Apply canvas filter for blur instead of StackBlur
            const originalFilterBlurredVideo = blurredVideoCtx.filter;
            if (blurAmount > 0) {
                blurredVideoCtx.filter = `blur(${blurAmount}px)`;
            } else {
                blurredVideoCtx.filter = 'none';
            }
            blurredVideoCtx.drawImage(video, 0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);
            blurredVideoCtx.filter = originalFilterBlurredVideo; // Reset filter

            // Ensure maskCanvas is sized correctly based on segmentation output (for skin)
            if (maskCanvas.width !== skinMaskWidth || maskCanvas.height !== skinMaskHeight) {
                maskCanvas.width = skinMaskWidth;
                maskCanvas.height = skinMaskHeight;
            }
            drawMaskFromPixelData(skinMaskPixelData, skinMaskWidth, skinMaskHeight, maskCanvasCtx, skinCategoryIndices); // Use skinCategoryIndices

            eyeMaskCtx.clearRect(0, 0, eyeMaskCanvas.width, eyeMaskCanvas.height);
            eyeMaskCtx.fillStyle = 'white';
            
            // Apply filter for blurring mask shapes
            const originalFilterEye = eyeMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                eyeMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }

            // Draw feature polygons (eyes, lips, eyebrows) onto eyeMaskCanvas
            function drawLandmarkPolygon(landmarkConnections, targetCtx, targetCanvas) { // Added targetCtx and targetCanvas
              if (landmarks && landmarkConnections && landmarkConnections.length > 0) {
                targetCtx.beginPath();
                const firstLandmarkIndex = landmarkConnections[0].start;
                if (landmarks[firstLandmarkIndex]) {
                  const firstPoint = landmarks[firstLandmarkIndex];
                  targetCtx.moveTo(firstPoint.x * targetCanvas.width, firstPoint.y * targetCanvas.height);
                  for (const conn of landmarkConnections) {
                    const endLandmarkIndex = conn.end;
                    if (landmarks[endLandmarkIndex]) {
                      const endPoint = landmarks[endLandmarkIndex];
                      targetCtx.lineTo(endPoint.x * targetCanvas.width, endPoint.y * targetCanvas.height);
                    } else {
                      console.warn(`Landmark index ${endLandmarkIndex} not found.`);
                    }
                  }
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Landmark index ${firstLandmarkIndex} not found for starting point.`);
                }
              }
            }

            function drawTriangleOnMask(vertexIndices, targetCtx, targetCanvas) { // Added targetCtx and targetCanvas
              if (landmarks && vertexIndices && vertexIndices.length === 3) {
                const points = vertexIndices.map(index => landmarks[index]);
                if (points.every(p => p)) {
                  targetCtx.beginPath();
                  targetCtx.moveTo(points[0].x * targetCanvas.width, points[0].y * targetCanvas.height);
                  targetCtx.lineTo(points[1].x * targetCanvas.width, points[1].y * targetCanvas.height);
                  targetCtx.lineTo(points[2].x * targetCanvas.width, points[2].y * targetCanvas.height);
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Invalid landmark index in triangle: ${vertexIndices}`);
                }
              }
            }

            // Draw features to be excluded from blur onto eyeMaskCanvas
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LIPS, eyeMaskCtx, eyeMaskCanvas);

            const innerMouthTriangles = [
              [78, 95, 88], [78, 178, 87], [95, 178, 96],
              [78, 88, 87], [88, 87, 178], [88, 95, 96],
              [96, 88, 178]
            ];
            innerMouthTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));
            drawTriangleOnMask([185, 292, 61], eyeMaskCtx, eyeMaskCanvas);
            const specificEyebrowTriangles = [
              [107, 55, 65], [63, 46, 52],
              [336, 285, 295], [293, 276, 334]
            ];
            specificEyebrowTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));

            eyeMaskCtx.filter = originalFilterEye; // Reset filter for eyeMaskCtx

            // Draw the face oval mask
            faceOvalMaskCtx.clearRect(0, 0, faceOvalMaskCanvas.width, faceOvalMaskCanvas.height);
            faceOvalMaskCtx.fillStyle = 'white';
            
            // Apply filter for blurring mask shape
            const originalFilterOval = faceOvalMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                faceOvalMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_FACE_OVAL, faceOvalMaskCtx, faceOvalMaskCanvas);
            faceOvalMaskCtx.filter = originalFilterOval; // Reset filter for faceOvalMaskCtx

            effectLayerCtx.clearRect(0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            // Start with the face-skin mask (from ImageSegmenter, skinCategoryIndices = [3])
            effectLayerCtx.drawImage(maskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            // Intersect with the face oval mask (keeps only skin within the face oval)
            effectLayerCtx.globalCompositeOperation = 'destination-in';
            effectLayerCtx.drawImage(faceOvalMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            // Subtract the eyes, lips, and eyebrows
            effectLayerCtx.globalCompositeOperation = 'destination-out';
            effectLayerCtx.drawImage(eyeMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            // Reset composite operation
            effectLayerCtx.globalCompositeOperation = 'source-over';

            // New: Clear the reused tempBlurLayerCtx
            tempBlurLayerCtx.clearRect(0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);

            // Draw the (smaller) blurredVideoCanvas, scaling it up to full size
            tempBlurLayerCtx.drawImage(blurredVideoCanvas, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
            tempBlurLayerCtx.globalCompositeOperation = 'destination-in';
            tempBlurLayerCtx.drawImage(effectLayerCanvas, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height); // REVERTED: Use effectLayerCanvas
            tempBlurLayerCtx.globalCompositeOperation = 'source-over';

            // Draw the smoothing effect layer onto the main canvas
            // This will be drawn on top of the person (who has a transparent background)
            canvasCtx.drawImage(tempBlurLayerCanvas, 0, 0);

          } else {
            console.warn("[predictWebcam] Skin mask data for smoothing was invalid or dimensions were zero. Skipping smoothing effect.");
          }
        } else {
            console.warn("[predictWebcam] No valid segmentation mask OR no landmarks found for this frame. Skipping smoothing effect.");
        }
      }
      
      if(segmentationResultsToClose){
        try {
            segmentationResultsToClose.close();
        } catch(e){
            console.warn("Error closing categoryMask:", e);
        }
      }

      if (isPredicting) {
        window.requestAnimationFrame(predictWebcam);
      } else {
        console.log("[predictWebcam END] Not predicting, loop will stop.");
      }
    }
  </script>
</body>
</html>
