<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Skin Smoothing with MediaPipe ImageSegmenter</title>
<style>
  html { /* New rule */
    height: 100%;
  }

  body {
    font-family: Arial, sans-serif;
    margin: 0; /* Changed from 20px */
    padding: 20px; /* Added for content spacing */
    box-sizing: border-box; /* Added for better sizing control */
    min-height: 100%; /* Ensure body takes full viewport height */
    display: flex;
    flex-direction: column;
    align-items: center;
    background-color: #131313; 
    background-image: url('sevBG.png');
    background-size: cover; 
    background-repeat: no-repeat; 
    background-position: center center;
    overflow-x: hidden; /* Added to prevent horizontal scroll on small overflows */
  }
  #liveViewContainer {
    position: relative;
    border: 1px solid #2b2b2b; 
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
    background-color: transparent; 
    padding: 10px;
    border-radius: 8px;
    width: 90%; /* Added for responsiveness */
    max-width: 700px; /* Added for a sensible max content width */
    display: flex; /* Added to help center canvas */
    justify-content: center; /* Added to center canvas */
    align-items: center; /* Added to center canvas */
  }
  video {
    display: none; /* Hide video element, we'll draw to canvas */
  }
  canvas#output_canvas { /* Selector made more specific */
    display: block;
    border: 1px solid #2b2b2b; 
    max-width: 100%; /* Added to ensure it scales down to fit container */
    height: auto;    /* Added to maintain aspect ratio when scaling */
  }
  .controls {
    margin-top: 15px;
    padding: 15px;
    background-color: #1f1f1f; 
    border-radius: 8px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 90%; /* Added for responsiveness */
    max-width: 700px; /* Added for a sensible max content width */
  }
  .controls button {
    padding: 10px 15px;
    font-size: 16px;
    cursor: pointer;
    border: none;
    border-radius: 5px;
    background-color: #a48353; /* Updated */
    color: #ffffff; /* Updated */
    margin-bottom: 10px;
  }
  .controls button:hover {
    background-color: #8c6f47; /* Darker shade of #a48353 for hover */
  }
  .controls label {
    margin-bottom: 5px;
    color: #ffffff; /* Updated */
  }
  .controls input[type="range"] {
    width: 200px;
    accent-color: #a48353; /* Updated */
  }
  #loadingMessage {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 1.2em;
    color: #131313; /* Updated */
    background-color: rgba(255, 255, 255, 0.9); /* Updated for better readability */
    padding: 10px;
    border-radius: 5px;
    display: none; /* Hidden by default */
  }
</style>
</head>
<body>
  <svg style="position:absolute; width:0; height:0; visibility:hidden;">
    <filter id="sharpenFilter">
      <feConvolveMatrix id="feConvMatrix" order="3 3" kernelMatrix="0 0 0 0 1 0 0 0 0" preserveAlpha="true"/>
    </filter>
  </svg>

  <div id="liveViewContainer">
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output_canvas" width="640" height="480"></canvas>
    <div id="loadingMessage">Loading model and webcam...</div>
  </div>

  <div class="controls">
    <button id="webcamButton">ENABLE WEBCAM</button>
    <button id="takePhotoButton" style="margin-top: 10px;">TAKE PHOTO</button>
    <label for="sharpenSlider">Sharpen Level:</label>
    <input type="range" id="sharpenSlider" min="0" max="1" step="0.01" value="0.2">
  </div>

  <script type="module">
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3"; // Changed import style
    const { ImageSegmenter, FaceLandmarker, FilesetResolver } = vision; // Destructure necessary components

    let imageSegmenter;
    let faceLandmarker;
    let runningMode = "VIDEO";
    const video = document.getElementById("webcam");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d", { alpha: true });
    const webcamButton = document.getElementById("webcamButton");
    const loadingMessage = document.getElementById("loadingMessage");
    const takePhotoButton = document.getElementById("takePhotoButton");
    const sharpenSlider = document.getElementById("sharpenSlider"); // Changed from blurSlider
    const feConvMatrixElement = document.getElementById("feConvMatrix"); // Get feConvolveMatrix element

    const blurScaleFactor = 0.5; 
    const landmarkMaskScaleFactor = 0.5; 
    const landmarkMaskBlurRadius = 3; 

    // Initial sharpenLevel set by the slider's default value
    let sharpenLevel = parseFloat(sharpenSlider.value);

    const blurredVideoCanvas = document.createElement("canvas");
    const blurredVideoCtx = blurredVideoCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const maskCanvas = document.createElement("canvas");
    const maskCanvasCtx = maskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const effectLayerCanvas = document.createElement("canvas");
    const effectLayerCtx = effectLayerCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const eyeMaskCanvas = document.createElement("canvas");
    const eyeMaskCtx = eyeMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const faceOvalMaskCanvas = document.createElement("canvas");
    const faceOvalMaskCtx = faceOvalMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    let tempBlurLayerCanvas;
    let tempBlurLayerCtx; // Will be initialized with willReadFrequently in enableCam

    const personMaskCanvas = document.createElement("canvas");
    const personMaskCtx = personMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const personOnlyCanvas = document.createElement("canvas");
    const personOnlyCtx = personOnlyCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const skinCategoryIndices = [3];
    const personCategoryIndices = [1, 2, 3];

    async function createMediaPipeTasks() {
      loadingMessage.textContent = 'Initializing MediaPipe tasks...';
      loadingMessage.style.display = 'block';
      console.log("Attempting to create MediaPipe tasks...");
      let visionFilesetResolver;
      try {
        visionFilesetResolver = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
      } catch (e) {
        console.error("Failed to load FilesetResolver for Vision Tasks:", e);
        loadingMessage.textContent = 'Error: Could not load MediaPipe vision tasks. Check console.';
        return;
      }
      console.log("FilesetResolver loaded.");

      let imageSegmenterDelegate = "GPU";
      try {
        loadingMessage.textContent = 'Loading Image Segmenter model (GPU)...';
        const segmenterStartTime = performance.now();
        imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
            delegate: "GPU"
          },
          runningMode: runningMode,
          outputCategoryMask: true,
          outputConfidenceMasks: false
        });
        const segmenterLoadTime = performance.now() - segmenterStartTime;
        console.log(`ImageSegmenter (GPU) created successfully in ${segmenterLoadTime.toFixed(2)}ms.`);
      } catch (gpuError) {
        console.warn("Failed to create ImageSegmenter with GPU delegate:", gpuError);
        imageSegmenterDelegate = "CPU";
        loadingMessage.textContent = 'GPU failed for Image Segmenter. Trying CPU...';
        try {
          const segmenterStartTime = performance.now();
          imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
            baseOptions: {
              modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
              delegate: "CPU"
            },
            runningMode: runningMode,
            outputCategoryMask: true,
            outputConfidenceMasks: false
          });
          const segmenterLoadTime = performance.now() - segmenterStartTime;
          console.log(`ImageSegmenter (CPU) created successfully in ${segmenterLoadTime.toFixed(2)}ms after GPU fallback.`);
        } catch (cpuError) {
          console.error("Failed to create ImageSegmenter with CPU delegate after GPU failure:", cpuError);
          loadingMessage.textContent = 'Error: Could not load Image Segmenter (GPU & CPU). Check console.';
          return;
        }
      }

      let faceLandmarkerDelegate = "GPU";
      try {
        loadingMessage.textContent = `Loading Face Landmarker model (GPU)... (Image Segmenter: ${imageSegmenterDelegate})`;
        const landmarkerStartTime = performance.now();
        faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: "GPU"
          },
          outputFaceBlendshapes: false,
          outputFacialTransformationMatrixes: false,
          runningMode: runningMode,
          numFaces: 1
        });
        const landmarkerLoadTime = performance.now() - landmarkerStartTime;
        console.log(`FaceLandmarker (GPU) created successfully in ${landmarkerLoadTime.toFixed(2)}ms.`);
      } catch (gpuError) {
        console.warn("Failed to create FaceLandmarker with GPU delegate:", gpuError);
        faceLandmarkerDelegate = "CPU";
        loadingMessage.textContent = `GPU failed for Face Landmarker. Trying CPU... (Image Segmenter: ${imageSegmenterDelegate})`;
        try {
          const landmarkerStartTime = performance.now();
          faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
            baseOptions: {
              modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
              delegate: "CPU"
            },
            outputFaceBlendshapes: false,
            outputFacialTransformationMatrixes: false,
            runningMode: runningMode,
            numFaces: 1
          });
          const landmarkerLoadTime = performance.now() - landmarkerStartTime;
          console.log(`FaceLandmarker (CPU) created successfully in ${landmarkerLoadTime.toFixed(2)}ms after GPU fallback.`);
        } catch (cpuError) {
          console.error("Failed to create FaceLandmarker with CPU delegate after GPU failure:", cpuError);
          loadingMessage.textContent = `Error: Could not load Face Landmarker (GPU & CPU). Image Segmenter: ${imageSegmenterDelegate}. Check console.`;
          return;
        }
      }
      
      loadingMessage.textContent = `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`;
      setTimeout(() => {
          if (loadingMessage.textContent === `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`) {
            loadingMessage.style.display = 'none';
          }
      }, 1500); // Increased timeout slightly
    }

    async function enableCam() {
      if (!imageSegmenter || !faceLandmarker) {
        loadingMessage.textContent = 'Models not yet loaded. Please wait...';
        loadingMessage.style.display = 'block';
        await createMediaPipeTasks();
        if (!imageSegmenter || !faceLandmarker) {
            console.error("MediaPipe tasks failed to initialize. Cannot enable webcam features.");
            webcamButton.disabled = false;
            return;
        }
      }

      if (webcamButton.textContent === "ENABLE WEBCAM") {
        webcamButton.disabled = true;
        loadingMessage.textContent = 'Accessing webcam...';
        loadingMessage.style.display = 'block';
        const constraints = { video: true };
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          video.srcObject = stream;
          
          const startPredictionLogic = () => {
            console.log(`Video 'playing' event fired. Raw dimensions: ${video.videoWidth}x${video.videoHeight}`);
            video.removeEventListener("playing", startPredictionLogic);

            if (video.videoWidth === 0 || video.videoHeight === 0) {
                console.error("CRITICAL: Video dimensions are zero at 'playing' event. Effect will likely fail.");
                loadingMessage.textContent = 'Error: Video stream has zero dimensions.';
                loadingMessage.style.display = 'block';
                webcamButton.disabled = false;
                isPredicting = false; 
                return;
            }
            
            canvasElement.width = video.videoWidth;
            canvasElement.height = video.videoHeight;
            
            blurredVideoCanvas.width = video.videoWidth * blurScaleFactor;
            blurredVideoCanvas.height = video.videoHeight * blurScaleFactor;
            
            effectLayerCanvas.width = video.videoWidth;
            effectLayerCanvas.height = video.videoHeight;
            eyeMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; 
            eyeMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; 
            faceOvalMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; 
            faceOvalMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; 

            personOnlyCanvas.width = video.videoWidth;
            personOnlyCanvas.height = video.videoHeight;

            if (!tempBlurLayerCanvas) {
                tempBlurLayerCanvas = document.createElement('canvas');
                tempBlurLayerCtx = tempBlurLayerCanvas.getContext('2d', {willReadFrequently: true, alpha: true});
            }
            tempBlurLayerCanvas.width = video.videoWidth;
            tempBlurLayerCanvas.height = video.videoHeight;

            loadingMessage.style.display = 'none';
            webcamButton.textContent = "DISABLE WEBCAM";
            webcamButton.disabled = false;
            
            console.log("[startPredictionLogic] Setting isPredicting = true.");
            isPredicting = true; 
            console.log("[startPredictionLogic] Calling predictWebcam for the first time.");
            predictWebcam();
          };

          video.addEventListener("playing", startPredictionLogic);
          
          video.play().then(() => {
            console.log("Video play() promise resolved. Playback should be starting or started.");
          }).catch(e => {
            console.error("Video play failed:", e);
            loadingMessage.textContent = 'Error playing video. Check permissions.';
            webcamButton.disabled = false;
            video.removeEventListener("playing", startPredictionLogic); 
          });

        } catch (err) {
          console.error("Error accessing webcam: ", err);
          loadingMessage.textContent = 'Error accessing webcam. Please allow access and try again.';
          webcamButton.disabled = false;
        }
      } else {
        video.srcObject.getTracks().forEach(track => track.stop());
        video.srcObject = null;
        webcamButton.textContent = "ENABLE WEBCAM";
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        isPredicting = false;
      }
    }
    webcamButton.addEventListener("click", enableCam);
    takePhotoButton.addEventListener("click", handleTakePhotoClick);

    sharpenSlider.addEventListener("input", (event) => { // Changed from blurSlider
      sharpenLevel = parseFloat(event.target.value); // sharpenLevel is 0 to 1
      const S = sharpenLevel;
      // Kernel for sharpen: 0 -S 0, -S 1+4S -S, 0 -S 0
      const kernel = `0 ${-S} 0 ${-S} ${1 + 4 * S} ${-S} 0 ${-S} 0`;
      feConvMatrixElement.setAttribute('kernelMatrix', kernel);
      console.log(`Sharpen slider changed. sharpenLevel: ${sharpenLevel}, Kernel: ${kernel}`);
    });

    // Initialize kernelMatrix on load based on initial slider value
    (function initializeSharpenFilter(){
        const initialS = sharpenLevel;
        const initialKernel = `0 ${-initialS} 0 ${-initialS} ${1 + 4 * initialS} ${-initialS} 0 ${-initialS} 0`;
        feConvMatrixElement.setAttribute('kernelMatrix', initialKernel);
        console.log(`Initial sharpenLevel: ${initialS}, Initial Kernel: ${initialKernel}`);
    })();

    async function dataURLtoBlob(dataurl) {
      const res = await fetch(dataurl);
      return await res.blob();
    }

    async function handleTakePhotoClick() {
      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        alert("Please enable the webcam and ensure it's active before taking a photo.");
        return;
      }
      if (canvasElement.width === 0 || canvasElement.height === 0) {
        alert("Canvas dimensions are zero. Cannot take photo. Please ensure webcam is properly initialized.");
        return;
      }

      try {
        const photoCanvas = document.createElement('canvas');
        photoCanvas.width = canvasElement.width;
        photoCanvas.height = canvasElement.height;
        const photoCtx = photoCanvas.getContext('2d');

        const dataUrl = await new Promise((resolve, reject) => {
          const backgroundImage = new Image();
          backgroundImage.src = 'sevBG.png';

          backgroundImage.onload = () => {
            const cw = photoCanvas.width;
            const ch = photoCanvas.height;
            const iw = backgroundImage.naturalWidth;
            const ih = backgroundImage.naturalHeight;

            const canvasRatio = cw / ch;
            const imageRatio = iw / ih;

            let dx = 0, dy = 0, dWidth = 0, dHeight = 0;

            if (canvasRatio > imageRatio) {
              dHeight = ch;
              dWidth = iw * (ch / ih);
              dx = (cw - dWidth) / 2;
              dy = 0;
            } else {
              dWidth = cw;
              dHeight = ih * (cw / iw);
              dx = 0;
              dy = (ch - dHeight) / 2;
            }

            photoCtx.drawImage(backgroundImage, dx, dy, dWidth, dHeight);

            photoCtx.drawImage(canvasElement, 0, 0, cw, ch);

            resolve(photoCanvas.toDataURL("image/png"));
          };

          backgroundImage.onerror = (err) => {
            console.error("Error loading background image 'sevBG.png'. Proceeding without custom background.", err);
            photoCtx.drawImage(canvasElement, 0, 0, photoCanvas.width, photoCanvas.height);
            resolve(photoCanvas.toDataURL("image/png"));
          };
        });

        const blob = await dataURLtoBlob(dataUrl);
        const fileName = "photo.png";
        const file = new File([blob], fileName, { type: "image/png" });
        
        const shareData = {
          files: [file],
          title: "My Photo",
          text: "Check out this photo I took!",
        };

        if (navigator.share && navigator.canShare && navigator.canShare(shareData)) {
          await navigator.share(shareData);
          console.log("Photo shared successfully or share dialog opened.");
        } else {
          console.warn("Web Share API not supported or cannot share this data. Falling back to download.");
          const link = document.createElement("a");
          link.href = dataUrl;
          link.download = fileName;
          document.body.appendChild(link);
          link.click();
          document.body.removeChild(link);
          alert("Photo downloaded as Web Share is not available.");
        }
      } catch (error) {
        console.error("Error taking or sharing photo:", error);
        alert(`Error: ${error.message}`);
        if (error.name !== 'AbortError') {
            const dataUrl = canvasElement.toDataURL("image/png");
            const link = document.createElement("a");
            link.href = dataUrl;
            link.download = "photo_fallback.png";
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            alert("Sharing failed. Photo downloaded instead.");
        }
      }
    }

    function drawMaskFromPixelData(maskPixelData, maskWidth, maskHeight, targetCtx, targetIndices) {
      if (!maskPixelData || maskPixelData.length === 0) {
        console.error("Invalid or empty maskPixelData received in drawMaskFromPixelData");
        targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
        return;
      }
      
      if (targetCtx.canvas.width !== maskWidth || targetCtx.canvas.height !== maskHeight) {
          targetCtx.canvas.width = maskWidth;
          targetCtx.canvas.height = maskHeight;
      }

      const newImageData = targetCtx.createImageData(maskWidth, maskHeight);
      const newPixelData = newImageData.data; 

      if (maskPixelData.length !== maskWidth * maskHeight) {
          console.error(`maskPixelData.length (${maskPixelData.length}) does not match maskWidth*maskHeight (${maskWidth*maskHeight}).`);
          targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
          return;
      }

      for (let i = 0; i < maskPixelData.length; i++) {
        const category = maskPixelData[i];
        const offset = i * 4;
        if (targetIndices.includes(category)) {
          newPixelData[offset] = 0;     // R (can be any color, alpha is key)
          newPixelData[offset + 1] = 0; // G
          newPixelData[offset + 2] = 0; // B
          newPixelData[offset + 3] = 255; // Alpha: Opaque for skin
        } else {
          newPixelData[offset + 3] = 0;   // Alpha: Transparent for non-skin
        }
      }
      targetCtx.putImageData(newImageData, 0, 0);
    }

    let lastVideoTime = -1;
    let isPredicting = false;

    async function predictWebcam() {
      console.log(`[predictWebcam START] isPredicting: ${isPredicting}, video.paused: ${video.paused}, video.ended: ${video.ended}, video.readyState: ${video.readyState}, sharpenLevel: ${sharpenLevel}`);

      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        isPredicting = false;
        console.log("[predictWebcam] Loop stopping: initial check failed (not predicting, no src, paused, or ended).");
        return;
      }

      if (video.readyState < video.HAVE_ENOUGH_DATA || !imageSegmenter || !faceLandmarker) {
          console.log(`[predictWebcam] Video not ready (readyState ${video.readyState}) or models not loaded. Retrying next frame.`);
          if (isPredicting) requestAnimationFrame(predictWebcam);
          return;
      }

      let startTimeMs = performance.now();
      let segmentationResultsToClose = null;

      if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        
        const segmentationResults = imageSegmenter.segmentForVideo(video, startTimeMs);
        if (segmentationResults && segmentationResults.categoryMask) {
            segmentationResultsToClose = segmentationResults.categoryMask;
        }
        console.log("Segmentation results:", segmentationResults);
        const landmarkResults = faceLandmarker.detectForVideo(video, startTimeMs);
        console.log("Landmark results:", landmarkResults);

        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

        let personDrawnSuccessfully = false;
        if (segmentationResults && segmentationResults.categoryMask) {
          const fullMask = segmentationResults.categoryMask;
          const fullMaskPixelData = fullMask.getAsUint8Array();
          const fullMaskWidth = fullMask.width;
          const fullMaskHeight = fullMask.height;

          if (fullMaskPixelData && fullMaskWidth > 0 && fullMaskHeight > 0) {
            if (personMaskCanvas.width !== fullMaskWidth || personMaskCanvas.height !== fullMaskHeight) {
              personMaskCanvas.width = fullMaskWidth;
              personMaskCanvas.height = fullMaskHeight;
            }
            drawMaskFromPixelData(fullMaskPixelData, fullMaskWidth, fullMaskHeight, personMaskCtx, personCategoryIndices);

            personOnlyCtx.clearRect(0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.drawImage(video, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.globalCompositeOperation = 'destination-in';
            personOnlyCtx.drawImage(personMaskCanvas, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.globalCompositeOperation = 'source-over';

            canvasCtx.drawImage(personOnlyCanvas, 0, 0, canvasElement.width, canvasElement.height);
            personDrawnSuccessfully = true;
          }
        }

        if (!personDrawnSuccessfully) {
          console.warn("[predictWebcam] Person segmentation failed or no mask. Drawing raw video.");
          canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);
        }

        if (segmentationResults && segmentationResults.categoryMask && landmarkResults && landmarkResults.faceLandmarks && landmarkResults.faceLandmarks.length > 0) {
          console.log("CategoryMask available for smoothing:", !!segmentationResults.categoryMask);
          console.log("FaceLandmarks available for smoothing:", landmarkResults.faceLandmarks.length);
          const skinMaskForSmoothing = segmentationResults.categoryMask;
          const skinMaskPixelData = skinMaskForSmoothing.getAsUint8Array();
          const skinMaskWidth = skinMaskForSmoothing.width;
          const skinMaskHeight = skinMaskForSmoothing.height;
          const landmarks = landmarkResults.faceLandmarks[0];
          console.log(`[iOS Debug] Skin mask dimensions for smoothing: ${skinMaskWidth}x${skinMaskHeight}. Pixel data length: ${skinMaskPixelData ? skinMaskPixelData.length : 'null'}`); 

          if (skinMaskPixelData && skinMaskWidth > 0 && skinMaskHeight > 0) {
            console.log("[iOS Debug] Valid skinMaskPixelData found. Proceeding with custom effect generation."); 
            // Draw video un-blurred onto the smaller blurredVideoCanvas (it now acts as a source)
            blurredVideoCtx.clearRect(0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);
            blurredVideoCtx.drawImage(video, 0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);

            if (maskCanvas.width !== skinMaskWidth || maskCanvas.height !== skinMaskHeight) {
                maskCanvas.width = skinMaskWidth;
                maskCanvas.height = skinMaskHeight;
            }
            drawMaskFromPixelData(skinMaskPixelData, skinMaskWidth, skinMaskHeight, maskCanvasCtx, skinCategoryIndices);

            eyeMaskCtx.clearRect(0, 0, eyeMaskCanvas.width, eyeMaskCanvas.height);
            eyeMaskCtx.fillStyle = 'white';
            
            const originalFilterEye = eyeMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                eyeMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }

            function drawLandmarkPolygon(landmarkConnections, targetCtx, targetCanvas) {
              if (landmarks && landmarkConnections && landmarkConnections.length > 0) {
                targetCtx.beginPath();
                const firstLandmarkIndex = landmarkConnections[0].start;
                if (landmarks[firstLandmarkIndex]) {
                  const firstPoint = landmarks[firstLandmarkIndex];
                  targetCtx.moveTo(firstPoint.x * targetCanvas.width, firstPoint.y * targetCanvas.height);
                  for (const conn of landmarkConnections) {
                    const endLandmarkIndex = conn.end;
                    if (landmarks[endLandmarkIndex]) {
                      const endPoint = landmarks[endLandmarkIndex];
                      targetCtx.lineTo(endPoint.x * targetCanvas.width, endPoint.y * targetCanvas.height);
                    } else {
                      console.warn(`Landmark index ${endLandmarkIndex} not found.`);
                    }
                  }
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Landmark index ${firstLandmarkIndex} not found for starting point.`);
                }
              }
            }

            function drawCustomLandmarkPolygonFromIndices(vertexIndices, targetCtx, targetCanvas) {
              if (landmarks && vertexIndices && vertexIndices.length > 0) {
                targetCtx.beginPath();
                const firstLandmarkIndex = vertexIndices[0];
                if (landmarks[firstLandmarkIndex]) {
                  const firstPoint = landmarks[firstLandmarkIndex];
                  targetCtx.moveTo(firstPoint.x * targetCanvas.width, firstPoint.y * targetCanvas.height);
                  for (let i = 1; i < vertexIndices.length; i++) {
                    const landmarkIndex = vertexIndices[i];
                    if (landmarks[landmarkIndex]) {
                      const point = landmarks[landmarkIndex];
                      targetCtx.lineTo(point.x * targetCanvas.width, point.y * targetCanvas.height);
                    } else {
                      console.warn(`Landmark index ${landmarkIndex} not found in custom list.`);
                    }
                  }
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Landmark index ${firstLandmarkIndex} not found for starting point in custom list.`);
                }
              }
            }

            function drawTriangleOnMask(vertexIndices, targetCtx, targetCanvas) {
              if (landmarks && vertexIndices && vertexIndices.length === 3) {
                const points = vertexIndices.map(index => landmarks[index]);
                if (points.every(p => p)) {
                  targetCtx.beginPath();
                  targetCtx.moveTo(points[0].x * targetCanvas.width, points[0].y * targetCanvas.height);
                  targetCtx.lineTo(points[1].x * targetCanvas.width, points[1].y * targetCanvas.height);
                  targetCtx.lineTo(points[2].x * targetCanvas.width, points[2].y * targetCanvas.height);
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Invalid landmark index in triangle: ${vertexIndices}`);
                }
              }
            }

            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LIPS, eyeMaskCtx, eyeMaskCanvas);

            const innerMouthTriangles = [
              [78, 95, 88], [78, 178, 87], [95, 178, 96],
              [78, 88, 87], [88, 87, 178], [88, 95, 96],
              [96, 88, 178]
            ];
            innerMouthTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));
            drawTriangleOnMask([185, 292, 61], eyeMaskCtx, eyeMaskCanvas);
            const specificEyebrowTriangles = [
              [107, 55, 65], [63, 46, 52],
              [336, 285, 295], [293, 276, 334]
            ];
            specificEyebrowTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));

            const additionalEyeVertices = [7,22,23,24,25,26,27,28,29,30,33,56,110,112,130,133,144,145,153,154,155,157,158,159,160,161,163,173,190,243,246,247];
            drawCustomLandmarkPolygonFromIndices(additionalEyeVertices, eyeMaskCtx, eyeMaskCanvas);

            const additionalOtherEyeVertices = [249,252,253,254,255,256,257,258,259,260,263,286,339,341,359,362,373,374,380,381,382,384,385,386,387,388,390,398,414,463,466,467];
            drawCustomLandmarkPolygonFromIndices(additionalOtherEyeVertices, eyeMaskCtx, eyeMaskCanvas);

            eyeMaskCtx.filter = originalFilterEye;

            faceOvalMaskCtx.clearRect(0, 0, faceOvalMaskCanvas.width, faceOvalMaskCanvas.height);
            faceOvalMaskCtx.fillStyle = 'white';
            
            const originalFilterOval = faceOvalMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                faceOvalMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_FACE_OVAL, faceOvalMaskCtx, faceOvalMaskCanvas);
            faceOvalMaskCtx.filter = originalFilterOval;

            effectLayerCtx.clearRect(0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            effectLayerCtx.drawImage(maskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'destination-in';
            effectLayerCtx.drawImage(faceOvalMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'destination-out';
            effectLayerCtx.drawImage(eyeMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'source-over';

            tempBlurLayerCtx.clearRect(0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);

            // Apply sharpen filter directly on the full-resolution tempBlurLayerCtx before drawing and masking
            const originalFilterTempBlur = tempBlurLayerCtx.filter;
            tempBlurLayerCtx.filter = 'none'; // Explicitly reset before applying new filter
            if (sharpenLevel > 0) { // Check sharpenLevel instead of blurAmount
                tempBlurLayerCtx.filter = 'url(#sharpenFilter)'; // Apply SVG sharpen filter
                console.log(`[Effect Debug] Applied sharpen filter. Level: ${sharpenLevel}`); 
            } else {
                console.log(`[Effect Debug] Sharpen level is 0 or less, no sharpen filter applied to tempBlurLayerCtx`); 
            }

            // Draw the (potentially scaled up) video frame from blurredVideoCanvas to tempBlurLayerCanvas.
            tempBlurLayerCtx.drawImage(blurredVideoCanvas, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);

            // Reset filter on tempBlurLayerCtx so it doesn't affect subsequent compositing operations
            tempBlurLayerCtx.filter = originalFilterTempBlur; 

            // Now apply the effect mask (skin mask, excluding eyes/mouth/oval)
            tempBlurLayerCtx.globalCompositeOperation = 'destination-in';
            tempBlurLayerCtx.drawImage(effectLayerCanvas, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
            tempBlurLayerCtx.globalCompositeOperation = 'source-over';

            canvasCtx.drawImage(tempBlurLayerCanvas, 0, 0, canvasElement.width, canvasElement.height);

          } else {
            console.warn("[iOS Debug] skinMaskPixelData for smoothing was INVALID or dimensions were zero. Skipping custom effect. This is a strong indicator why effect might not be working on iOS."); 
          }
        } else {
            console.warn("[iOS Debug] No valid segmentation mask OR no landmarks found for this frame. Skipping custom effect."); 
        }
      }
      
      if(segmentationResultsToClose){
        try {
            segmentationResultsToClose.close();
        } catch(e){
            console.warn("Error closing categoryMask:", e);
        }
      }

      if (isPredicting) {
        window.requestAnimationFrame(predictWebcam);
      } else {
        console.log("[predictWebcam END] Not predicting, loop will stop.");
      }
    }
  </script>
</body>
</html>
