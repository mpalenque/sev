<!DOCTYPE html>
<html>
<head>
<meta charset="utf-8">
<title>Skin Smoothing with MediaPipe ImageSegmenter</title>
<style>
  html { /* New rule */
    height: 100%;
  }

  body {
    font-family: Arial, sans-serif;
    margin: 0; /* Changed from 20px */
    padding: 20px; /* Added for content spacing */
    box-sizing: border-box; /* Added for better sizing control */
    min-height: 100%; /* Ensure body takes full viewport height */
    display: flex;
    flex-direction: column;
    align-items: center;
    background-color: #131313;
    background-image: url('sevBG.png');
    background-size: cover;
    background-repeat: no-repeat;
    background-position: 10% center; /* MODIFIED: Shifted 40% left from center */
    overflow: hidden; /* Prevent all scrolling on the body */
  }
  #liveViewContainer {
    position: relative;
    /* border: 1px solid #2b2b2b; */ /* MODIFIED: Removed border */
    /* box-shadow: 0 4px 8px rgba(0,0,0,0.1); */ /* MODIFIED: Optionally remove shadow if it looks like a frame */
    background-color: transparent;
    padding: 10px;
    border-radius: 8px;
    width: 90%; /* Added for responsiveness */
    max-width: 700px; /* Added for a sensible max content width */
    display: flex; /* Added to help center canvas */
    justify-content: center; /* Added to center canvas */
    align-items: center; /* Added to center canvas */
  }
  video {
    display: none; /* Hide video element, we'll draw to canvas */
  }
  canvas#output_canvas { /* Selector made more specific */
    display: block;
    /* border: 1px solid #2b2b2b; */ /* MODIFIED: Removed border */
    max-width: 100%; /* Added to ensure it scales down to fit container */
    height: auto;    /* Added to maintain aspect ratio when scaling */
  }
  .controls {
    margin-top: 15px;
    padding: 15px;
    /* background-color: #1f1f1f; */ /* MODIFIED: Made transparent */
    background-color: transparent; /* MODIFIED: Made transparent */
    border-radius: 8px;
    /* box-shadow: 0 2px 4px rgba(0,0,0,0.1); */ /* MODIFIED: Removed shadow */
    display: flex;
    flex-direction: column;
    align-items: center;
    width: 90%; /* Added for responsiveness */
    max-width: 700px; /* Added for a sensible max content width */
  }
  .controls button {
    padding: 10px 15px;
    font-size: 16px;
    cursor: pointer;
    border: none;
    border-radius: 5px;
    background-color: #a48353; /* Updated */
    color: #ffffff; /* Updated */
    margin-bottom: 10px;
  }
  .controls button:hover {
    background-color: #8c6f47; /* Darker shade of #a48353 for hover */
  }
  .controls label {
    margin-bottom: 5px;
    color: #ffffff; /* Updated */
  }
  .controls input[type="range"] {
    width: 250px; /* Increased width */
    height: 20px; /* Increased height for the track */
    accent-color: #a48353; 
    margin-top: 10px; /* Added margin for better spacing */
    margin-bottom: 10px; /* Added margin for better spacing */
    padding: 10px 0; /* Add padding to make the touch area for the track larger vertically */
    box-sizing: content-box; /* Ensure padding adds to height for touch purposes */
  }

  /* Styles for WebKit (Chrome, Safari, newer Edge) slider thumb */
  .controls input[type="range"]::-webkit-slider-thumb {
    -webkit-appearance: none; /* Important to override default styling */
    appearance: none;
    width: 40px; /* Further Increased thumb width */
    height: 40px; /* Further Increased thumb height */
    background: #a48353;
    cursor: pointer;
    border-radius: 50%; /* Circular thumb */
    border: 2px solid white; /* Optional: adds a border to the thumb */
    /* Adjust thumb position to align with a taller track if needed 
       e.g., margin-top: -10px; if track height + padding is tall */
  }

  /* Styles for Firefox slider thumb */
  .controls input[type="range"]::-moz-range-thumb {
    width: 40px; /* Further Increased thumb width */
    height: 40px; /* Further Increased thumb height */
    background: #a48353;
    cursor: pointer;
    border-radius: 50%;
    border: 2px solid white; /* Optional: adds a border to the thumb */
  }

  #loadingMessage {
    position: absolute;
    top: 50%;
    left: 50%;
    transform: translate(-50%, -50%);
    font-size: 1.2em;
    color: #131313; /* Updated */
    background-color: rgba(255, 255, 255, 0.9); /* Updated for better readability */
    padding: 10px;
    border-radius: 5px;
    display: none; /* Hidden by default */
  }
</style>
</head>
<body>
  <div id="liveViewContainer">
    <video id="webcam" autoplay playsinline></video>
    <canvas id="output_canvas" width="640" height="480"></canvas>
    <div id="loadingMessage">Loading model and webcam...</div>
  </div>

  <div class="controls">
    <button id="webcamButton">ENABLE WEBCAM</button>
    <button id="takePhotoButton" style="margin-top: 10px;">TAKE PHOTO</button>
    <label for="sharpenSlider" style="display: none;">Sharpen Level:</label> <!-- MODIFIED: Hidden -->
    <input type="range" id="sharpenSlider" min="0" max="1" step="0.01" value="0.2" style="display: none;"> <!-- MODIFIED: Hidden -->
  </div>

  <script type="module">
    import vision from "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3"; // Changed import style
    const { ImageSegmenter, FaceLandmarker, FilesetResolver } = vision; // Destructure necessary components

    let imageSegmenter;
    let faceLandmarker;
    let runningMode = "VIDEO";
    const video = document.getElementById("webcam");
    const canvasElement = document.getElementById("output_canvas");
    const canvasCtx = canvasElement.getContext("2d", { alpha: true });
    const webcamButton = document.getElementById("webcamButton");
    const loadingMessage = document.getElementById("loadingMessage");
    const takePhotoButton = document.getElementById("takePhotoButton");
    const sharpenSlider = document.getElementById("sharpenSlider"); 

    const blurScaleFactor = 0.5; 
    const landmarkMaskScaleFactor = 0.5; 
    const landmarkMaskBlurRadius = 0; // Changed from 3 to 0 to remove blur from exclusion masks

    let sharpenLevel = parseFloat(sharpenSlider.value);

    const blurredVideoCanvas = document.createElement("canvas");
    const blurredVideoCtx = blurredVideoCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const maskCanvas = document.createElement("canvas");
    const maskCanvasCtx = maskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const effectLayerCanvas = document.createElement("canvas");
    const effectLayerCtx = effectLayerCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const eyeMaskCanvas = document.createElement("canvas");
    const eyeMaskCtx = eyeMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const faceOvalMaskCanvas = document.createElement("canvas");
    const faceOvalMaskCtx = faceOvalMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    let tempBlurLayerCanvas;
    let tempBlurLayerCtx; // Will be initialized with willReadFrequently in enableCam

    let sharpenedSmallCanvas; // Added for JS sharpen
    let sharpenedSmallCtx;    // Added for JS sharpen

    const personMaskCanvas = document.createElement("canvas");
    const personMaskCtx = personMaskCanvas.getContext("2d", { alpha: true, willReadFrequently: true });
    const personOnlyCanvas = document.createElement("canvas");
    const personOnlyCtx = personOnlyCanvas.getContext("2d", { alpha: true, willReadFrequently: true });

    const skinCategoryIndices = [3];
    const personCategoryIndices = [1, 2, 3, 4, 5]; // MODIFIED: Added 5 (others)

    // Function to detect iOS
    function isIOS() {
      return /iPad|iPhone|iPod/.test(navigator.userAgent) && !window.MSStream;
    }

    async function createMediaPipeTasks() {
      loadingMessage.textContent = 'Initializing MediaPipe tasks...';
      loadingMessage.style.display = 'block';
      console.log("Attempting to create MediaPipe tasks...");
      const runningOnIOS = isIOS();
      console.log("Running on iOS:", runningOnIOS);

      let visionFilesetResolver;
      try {
        visionFilesetResolver = await FilesetResolver.forVisionTasks(
          "https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm"
        );
      } catch (e) {
        console.error("Failed to load FilesetResolver for Vision Tasks:", e);
        loadingMessage.textContent = 'Error: Could not load MediaPipe vision tasks. Check console.';
        return;
      }
      console.log("FilesetResolver loaded.");

      let imageSegmenterDelegate = runningOnIOS ? "CPU" : "GPU";
      try {
        loadingMessage.textContent = `Loading Image Segmenter model (${imageSegmenterDelegate})...`;
        const segmenterStartTime = performance.now();
        imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
            delegate: imageSegmenterDelegate
          },
          runningMode: runningMode,
          outputCategoryMask: true,
          outputConfidenceMasks: false
        });
        const segmenterLoadTime = performance.now() - segmenterStartTime;
        console.log(`ImageSegmenter (${imageSegmenterDelegate}) created successfully in ${segmenterLoadTime.toFixed(2)}ms.`);
      } catch (initialError) {
        console.warn(`Failed to create ImageSegmenter with ${imageSegmenterDelegate} delegate:`, initialError);
        if (!runningOnIOS && imageSegmenterDelegate === "GPU") { // Only try CPU if initial attempt was GPU and not on iOS
          imageSegmenterDelegate = "CPU";
          loadingMessage.textContent = 'GPU failed for Image Segmenter. Trying CPU...';
          try {
            const segmenterStartTime = performance.now();
            imageSegmenter = await ImageSegmenter.createFromOptions(visionFilesetResolver, {
              baseOptions: {
                modelAssetPath: "https://storage.googleapis.com/mediapipe-models/image_segmenter/selfie_multiclass_256x256/float32/latest/selfie_multiclass_256x256.tflite",
                delegate: "CPU"
              },
              runningMode: runningMode,
              outputCategoryMask: true,
              outputConfidenceMasks: false
            });
            const segmenterLoadTime = performance.now() - segmenterStartTime;
            console.log(`ImageSegmenter (CPU) created successfully in ${segmenterLoadTime.toFixed(2)}ms after GPU fallback.`);
          } catch (cpuError) {
            console.error("Failed to create ImageSegmenter with CPU delegate after GPU failure:", cpuError);
            loadingMessage.textContent = 'Error: Could not load Image Segmenter (GPU & CPU). Check console.';
            return;
          }
        } else { // If on iOS and CPU failed, or if CPU was already tried
            console.error(`Failed to create ImageSegmenter with ${imageSegmenterDelegate} delegate:`, initialError);
            loadingMessage.textContent = `Error: Could not load Image Segmenter (${imageSegmenterDelegate}). Check console.`;
            return;
        }
      }

      let faceLandmarkerDelegate = runningOnIOS ? "CPU" : "GPU";
      try {
        loadingMessage.textContent = `Loading Face Landmarker model (${faceLandmarkerDelegate})... (Image Segmenter: ${imageSegmenterDelegate})`;
        const landmarkerStartTime = performance.now();
        faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
          baseOptions: {
            modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
            delegate: faceLandmarkerDelegate
          },
          outputFaceBlendshapes: false,
          outputFacialTransformationMatrixes: false,
          runningMode: runningMode,
          numFaces: 1
        });
        const landmarkerLoadTime = performance.now() - landmarkerStartTime;
        console.log(`FaceLandmarker (${faceLandmarkerDelegate}) created successfully in ${landmarkerLoadTime.toFixed(2)}ms.`);
      } catch (initialError) {
        console.warn(`Failed to create FaceLandmarker with ${faceLandmarkerDelegate} delegate:`, initialError);
        if (!runningOnIOS && faceLandmarkerDelegate === "GPU") { // Only try CPU if initial attempt was GPU and not on iOS
          faceLandmarkerDelegate = "CPU";
          loadingMessage.textContent = `GPU failed for Face Landmarker. Trying CPU... (Image Segmenter: ${imageSegmenterDelegate})`;
          try {
            const landmarkerStartTime = performance.now();
            faceLandmarker = await FaceLandmarker.createFromOptions(visionFilesetResolver, {
              baseOptions: {
                modelAssetPath: `https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task`,
                delegate: "CPU"
              },
              outputFaceBlendshapes: false,
              outputFacialTransformationMatrixes: false,
              runningMode: runningMode,
              numFaces: 1
            });
            const landmarkerLoadTime = performance.now() - landmarkerStartTime;
            console.log(`FaceLandmarker (CPU) created successfully in ${landmarkerLoadTime.toFixed(2)}ms after GPU fallback.`);
          } catch (cpuError) {
            console.error("Failed to create FaceLandmarker with CPU delegate after GPU failure:", cpuError);
            loadingMessage.textContent = `Error: Could not load Face Landmarker (GPU & CPU). Image Segmenter: ${imageSegmenterDelegate}. Check console.`;
            return;
          }
        } else { // If on iOS and CPU failed, or if CPU was already tried
            console.error(`Failed to create FaceLandmarker with ${faceLandmarkerDelegate} delegate:`, initialError);
            loadingMessage.textContent = `Error: Could not load Face Landmarker (${faceLandmarkerDelegate}). Image Segmenter: ${imageSegmenterDelegate}. Check console.`;
            return;
        }
      }
      
      loadingMessage.textContent = `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`;
      setTimeout(() => {
          if (loadingMessage.textContent === `Models loaded (Segmenter: ${imageSegmenterDelegate}, Landmarker: ${faceLandmarkerDelegate}).`) {
            loadingMessage.style.display = 'none';
          }
      }, 1500); // Increased timeout slightly
    }

    async function enableCam() {
      if (!imageSegmenter || !faceLandmarker) {
        loadingMessage.textContent = 'Models not yet loaded. Please wait...';
        loadingMessage.style.display = 'block';
        await createMediaPipeTasks();
        if (!imageSegmenter || !faceLandmarker) {
            console.error("MediaPipe tasks failed to initialize. Cannot enable webcam features.");
            webcamButton.disabled = false;
            return;
        }
      }

      if (webcamButton.textContent === "ENABLE WEBCAM") {
        webcamButton.disabled = true;
        loadingMessage.textContent = 'Accessing webcam...';
        loadingMessage.style.display = 'block';
        const constraints = { video: true };
        try {
          const stream = await navigator.mediaDevices.getUserMedia(constraints);
          video.srcObject = stream;
          
          const startPredictionLogic = () => {
            console.log(`Video 'playing' event fired. Raw dimensions: ${video.videoWidth}x${video.videoHeight}`);
            video.removeEventListener("playing", startPredictionLogic);

            if (video.videoWidth === 0 || video.videoHeight === 0) {
                console.error("CRITICAL: Video dimensions are zero at 'playing' event. Effect will likely fail.");
                loadingMessage.textContent = 'Error: Video stream has zero dimensions.';
                loadingMessage.style.display = 'block';
                webcamButton.disabled = false;
                isPredicting = false; 
                return;
            }
            
            canvasElement.width = video.videoWidth;
            canvasElement.height = video.videoHeight;
            
            blurredVideoCanvas.width = video.videoWidth * blurScaleFactor;
            blurredVideoCanvas.height = video.videoHeight * blurScaleFactor;
            
            effectLayerCanvas.width = video.videoWidth;
            effectLayerCanvas.height = video.videoHeight;
            eyeMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; 
            eyeMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; 
            faceOvalMaskCanvas.width = video.videoWidth * landmarkMaskScaleFactor; 
            faceOvalMaskCanvas.height = video.videoHeight * landmarkMaskScaleFactor; 

            personOnlyCanvas.width = video.videoWidth;
            personOnlyCanvas.height = video.videoHeight;

            if (!tempBlurLayerCanvas) {
                tempBlurLayerCanvas = document.createElement('canvas');
                tempBlurLayerCtx = tempBlurLayerCanvas.getContext('2d', {willReadFrequently: true, alpha: true});
            }
            tempBlurLayerCanvas.width = video.videoWidth;
            tempBlurLayerCanvas.height = video.videoHeight;

            // Initialize sharpenedSmallCanvas
            if (!sharpenedSmallCanvas) {
                sharpenedSmallCanvas = document.createElement('canvas');
                sharpenedSmallCtx = sharpenedSmallCanvas.getContext('2d'); // willReadFrequently not strictly needed if only doing putImageData
            }
            sharpenedSmallCanvas.width = blurredVideoCanvas.width; // Same small dimensions as blurredVideoCanvas
            sharpenedSmallCanvas.height = blurredVideoCanvas.height;

            loadingMessage.style.display = 'none';
            webcamButton.textContent = "DISABLE WEBCAM";
            webcamButton.disabled = false;
            
            console.log("[startPredictionLogic] Setting isPredicting = true.");
            isPredicting = true; 
            console.log("[startPredictionLogic] Calling predictWebcam for the first time.");
            predictWebcam();
          };

          video.addEventListener("playing", startPredictionLogic);
          
          video.play().then(() => {
            console.log("Video play() promise resolved. Playback should be starting or started.");
          }).catch(e => {
            console.error("Video play failed:", e);
            loadingMessage.textContent = 'Error playing video. Check permissions.';
            webcamButton.disabled = false;
            video.removeEventListener("playing", startPredictionLogic); 
          });

        } catch (err) {
          console.error("Error accessing webcam: ", err);
          loadingMessage.textContent = 'Error accessing webcam. Please allow access and try again.';
          webcamButton.disabled = false;
        }
      } else {
        video.srcObject.getTracks().forEach(track => track.stop());
        video.srcObject = null;
        webcamButton.textContent = "ENABLE WEBCAM";
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        isPredicting = false;
      }
    }
    webcamButton.addEventListener("click", enableCam);
    takePhotoButton.addEventListener("click", handleTakePhotoClick);

    sharpenSlider.addEventListener("input", (event) => { 
      sharpenLevel = parseFloat(event.target.value); // sharpenLevel is 0 to 1
      console.log(`Sharpen slider changed. sharpenLevel: ${sharpenLevel}`);
    });

    async function dataURLtoBlob(dataurl) {
      const res = await fetch(dataurl);
      return await res.blob();
    }

    async function handleTakePhotoClick() {
      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        alert("Please enable the webcam and ensure it's active before taking a photo.");
        return;
      }
      if (canvasElement.width === 0 || canvasElement.height === 0) {
        alert("Canvas dimensions are zero. Cannot take photo. Please ensure webcam is properly initialized.");
        return;
      }

      try {
        const photoCanvas = document.createElement('canvas');
        photoCanvas.width = canvasElement.width;
        photoCanvas.height = canvasElement.height;
        const photoCtx = photoCanvas.getContext('2d');

        const dataUrl = await new Promise((resolve, reject) => {
          const backgroundImage = new Image();
          backgroundImage.src = 'sevBG.png';

          backgroundImage.onload = () => {
            const cw = photoCanvas.width;
            const ch = photoCanvas.height;
            const iw = backgroundImage.naturalWidth;
            const ih = backgroundImage.naturalHeight;

            const canvasRatio = cw / ch;
            const imageRatio = iw / ih;

            let dx = 0, dy = 0, dWidth = 0, dHeight = 0;

            if (canvasRatio > imageRatio) {
              dHeight = ch;
              dWidth = iw * (ch / ih);
              dx = (cw - dWidth) / 2;
              dy = 0;
            } else {
              dWidth = cw;
              dHeight = ih * (cw / iw);
              dx = 0;
              dy = (ch - dHeight) / 2;
            }

            photoCtx.drawImage(backgroundImage, dx, dy, dWidth, dHeight);

            photoCtx.drawImage(canvasElement, 0, 0, cw, ch);

            resolve(photoCanvas.toDataURL("image/png"));
          };

          backgroundImage.onerror = (err) => {
            console.error("Error loading background image 'sevBG.png'. Proceeding without custom background.", err);
            photoCtx.drawImage(canvasElement, 0, 0, photoCanvas.width, photoCanvas.height);
            resolve(photoCanvas.toDataURL("image/png"));
          };
        });

        const blob = await dataURLtoBlob(dataUrl);
        const fileName = "photo.png";
        const file = new File([blob], fileName, { type: "image/png" });
        
        const shareData = {
          files: [file],
          title: "My Photo",
          text: "Check out this photo I took!",
        };

        if (navigator.share && navigator.canShare && navigator.canShare(shareData)) {
          await navigator.share(shareData);
          console.log("Photo shared successfully or share dialog opened.");
        } else {
          console.warn("Web Share API not supported or cannot share this data. Falling back to download.");
          const link = document.createElement("a");
          link.href = dataUrl;
          link.download = fileName;
          document.body.appendChild(link);
          link.click();
          document.body.removeChild(link);
          alert("Photo downloaded as Web Share is not available.");
        }
      } catch (error) {
        console.error("Error taking or sharing photo:", error);
        alert(`Error: ${error.message}`);
        if (error.name !== 'AbortError') {
            const dataUrl = canvasElement.toDataURL("image/png");
            const link = document.createElement("a");
            link.href = dataUrl;
            link.download = "photo_fallback.png";
            document.body.appendChild(link);
            link.click();
            document.body.removeChild(link);
            alert("Sharing failed. Photo downloaded instead.");
        }
      }
    }

    function drawMaskFromPixelData(maskPixelData, maskWidth, maskHeight, targetCtx, targetIndices) {
      if (!maskPixelData || maskPixelData.length === 0) {
        console.error("Invalid or empty maskPixelData received in drawMaskFromPixelData");
        targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
        return;
      }
      
      if (targetCtx.canvas.width !== maskWidth || targetCtx.canvas.height !== maskHeight) {
          targetCtx.canvas.width = maskWidth;
          targetCtx.canvas.height = maskHeight;
      }

      const newImageData = targetCtx.createImageData(maskWidth, maskHeight);
      const newPixelData = newImageData.data; 

      if (maskPixelData.length !== maskWidth * maskHeight) {
          console.error(`maskPixelData.length (${maskPixelData.length}) does not match maskWidth*maskHeight (${maskWidth*maskHeight}).`);
          targetCtx.clearRect(0, 0, targetCtx.canvas.width, targetCtx.canvas.height);
          return;
      }

      for (let i = 0; i < maskPixelData.length; i++) {
        const category = maskPixelData[i];
        const offset = i * 4;
        if (targetIndices.includes(category)) {
          newPixelData[offset] = 0;     // R (can be any color, alpha is key)
          newPixelData[offset + 1] = 0; // G
          newPixelData[offset + 2] = 0; // B
          newPixelData[offset + 3] = 255; // Alpha: Opaque for skin
        } else {
          newPixelData[offset + 3] = 0;   // Alpha: Transparent for non-skin
        }
      }
      targetCtx.putImageData(newImageData, 0, 0);
    }

    function applySharpenEffectJS(sourceCtx, sharpenAmount) {
      const width = sourceCtx.canvas.width;
      const height = sourceCtx.canvas.height;
      const S = sharpenAmount;
      console.log(`[JS Sharpen] applySharpenEffectJS called. Width: ${width}, Height: ${height}, SharpenAmount (S): ${S}`);

      const srcData = sourceCtx.getImageData(0, 0, width, height);
      console.log(`[JS Sharpen] Got source ImageData. Length: ${srcData.data.length}`);

      if (S === 0) {
        console.log("[JS Sharpen] S is 0, returning original data.");
        return srcData;
      }

      const srcPixels = srcData.data;
      const dstImageData = sourceCtx.createImageData(width, height); 
      const dstPixels = dstImageData.data;

      // Kernel for sharpen: 0 -S 0, -S 1+4S -S, 0 -S 0
      const kernel = [
          [0, -S, 0],
          [-S, 1 + 4 * S, -S],
          [0, -S, 0]
      ];

      const sampleX = Math.floor(width / 2);
      const sampleY = Math.floor(height / 2);
      const sampleIndex = (sampleY * width + sampleX) * 4;

      if (srcPixels.length > sampleIndex + 3) {
        console.log(`[JS Sharpen] Before - Sample pixel at [${sampleX},${sampleY}] (R,G,B,A): ${srcPixels[sampleIndex]}, ${srcPixels[sampleIndex+1]}, ${srcPixels[sampleIndex+2]}, ${srcPixels[sampleIndex+3]}`);
      }

      for (let y = 0; y < height; y++) {
          for (let x = 0; x < width; x++) {
              let r = 0, g = 0, b = 0;
              const baseOutputIndex = (y * width + x) * 4;

              for (let ky = -1; ky <= 1; ky++) {
                  for (let kx = -1; kx <= 1; kx++) {
                      const currentKernelVal = kernel[ky + 1][kx + 1];
                      if (currentKernelVal === 0) continue; 

                      const pixelY = y + ky;
                      const pixelX = x + kx;

                      // Clamp coordinates to be within bounds (edge handling)
                      const clampedY = Math.max(0, Math.min(height - 1, pixelY));
                      const clampedX = Math.max(0, Math.min(width - 1, pixelX));
                      
                      const srcOffset = (clampedY * width + clampedX) * 4;
                      
                      r += srcPixels[srcOffset] * currentKernelVal;
                      g += srcPixels[srcOffset + 1] * currentKernelVal;
                      b += srcPixels[srcOffset + 2] * currentKernelVal;
                  }
              }

              dstPixels[baseOutputIndex] = Math.max(0, Math.min(255, r));
              dstPixels[baseOutputIndex + 1] = Math.max(0, Math.min(255, g));
              dstPixels[baseOutputIndex + 2] = Math.max(0, Math.min(255, b));
              dstPixels[baseOutputIndex + 3] = srcPixels[baseOutputIndex + 3]; // Preserve alpha
          }
      }

      if (dstPixels.length > sampleIndex + 3) {
        console.log(`[JS Sharpen] After - Sample pixel at [${sampleX},${sampleY}] (R,G,B,A): ${dstPixels[sampleIndex]}, ${dstPixels[sampleIndex+1]}, ${dstPixels[sampleIndex+2]}, ${dstPixels[sampleIndex+3]}`);
      }
      console.log("[JS Sharpen] Convolution finished, returning destination ImageData.");
      return dstImageData;
    }

    let lastVideoTime = -1;
    let isPredicting = false;

    async function predictWebcam() {
      console.log(`[predictWebcam START] isPredicting: ${isPredicting}, video.paused: ${video.paused}, video.ended: ${video.ended}, video.readyState: ${video.readyState}, sharpenLevel: ${sharpenLevel}`);

      if (!isPredicting || !video.srcObject || video.paused || video.ended) {
        isPredicting = false;
        console.log("[predictWebcam] Loop stopping: initial check failed (not predicting, no src, paused, or ended).");
        return;
      }

      if (video.readyState < video.HAVE_ENOUGH_DATA || !imageSegmenter || !faceLandmarker) {
          console.log(`[predictWebcam] Video not ready (readyState ${video.readyState}) or models not loaded. Retrying next frame.`);
          if (isPredicting) requestAnimationFrame(predictWebcam);
          return;
      }

      let startTimeMs = performance.now();
      let segmentationResultsToClose = null;

      if (video.currentTime !== lastVideoTime) {
        lastVideoTime = video.currentTime;
        
        const segmentationResults = imageSegmenter.segmentForVideo(video, startTimeMs);
        if (segmentationResults && segmentationResults.categoryMask) {
            segmentationResultsToClose = segmentationResults.categoryMask;
        }
        console.log("Segmentation results:", segmentationResults);
        const landmarkResults = faceLandmarker.detectForVideo(video, startTimeMs);
        console.log("Landmark results:", landmarkResults);

        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);

        let personDrawnSuccessfully = false;
        if (segmentationResults && segmentationResults.categoryMask) {
          const fullMask = segmentationResults.categoryMask;
          const fullMaskPixelData = fullMask.getAsUint8Array();
          const fullMaskWidth = fullMask.width;
          const fullMaskHeight = fullMask.height;

          if (fullMaskPixelData && fullMaskWidth > 0 && fullMaskHeight > 0) {
            if (personMaskCanvas.width !== fullMaskWidth || personMaskCanvas.height !== fullMaskHeight) {
              personMaskCanvas.width = fullMaskWidth;
              personMaskCanvas.height = fullMaskHeight;
            }
            drawMaskFromPixelData(fullMaskPixelData, fullMaskWidth, fullMaskHeight, personMaskCtx, personCategoryIndices);

            personOnlyCtx.clearRect(0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.drawImage(video, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.globalCompositeOperation = 'destination-in';
            personOnlyCtx.drawImage(personMaskCanvas, 0, 0, personOnlyCanvas.width, personOnlyCanvas.height);
            personOnlyCtx.globalCompositeOperation = 'source-over';

            canvasCtx.drawImage(personOnlyCanvas, 0, 0, canvasElement.width, canvasElement.height);
            personDrawnSuccessfully = true;
          }
        }

        if (!personDrawnSuccessfully) {
          console.warn("[predictWebcam] Person segmentation failed or no mask. Drawing raw video.");
          canvasCtx.drawImage(video, 0, 0, canvasElement.width, canvasElement.height);
        }

        if (segmentationResults && segmentationResults.categoryMask && landmarkResults && landmarkResults.faceLandmarks && landmarkResults.faceLandmarks.length > 0) {
          console.log("CategoryMask available for smoothing:", !!segmentationResults.categoryMask);
          console.log("FaceLandmarks available for smoothing:", landmarkResults.faceLandmarks.length);
          const skinMaskForSmoothing = segmentationResults.categoryMask;
          const skinMaskPixelData = skinMaskForSmoothing.getAsUint8Array();
          const skinMaskWidth = skinMaskForSmoothing.width;
          const skinMaskHeight = skinMaskForSmoothing.height;
          const landmarks = landmarkResults.faceLandmarks[0];
          console.log(`[iOS Debug] Skin mask dimensions for smoothing: ${skinMaskWidth}x${skinMaskHeight}. Pixel data length: ${skinMaskPixelData ? skinMaskPixelData.length : 'null'}`); 

          if (skinMaskPixelData && skinMaskWidth > 0 && skinMaskHeight > 0) {
            console.log("[iOS Debug] Valid skinMaskPixelData found. Proceeding with custom effect generation."); 
            // Draw video un-blurred onto the smaller blurredVideoCanvas (it now acts as a source)
            blurredVideoCtx.clearRect(0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);
            blurredVideoCtx.drawImage(video, 0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);

            if (maskCanvas.width !== skinMaskWidth || maskCanvas.height !== skinMaskHeight) {
                maskCanvas.width = skinMaskWidth;
                maskCanvas.height = skinMaskHeight;
            }
            drawMaskFromPixelData(skinMaskPixelData, skinMaskWidth, skinMaskHeight, maskCanvasCtx, skinCategoryIndices);

            eyeMaskCtx.clearRect(0, 0, eyeMaskCanvas.width, eyeMaskCanvas.height);
            eyeMaskCtx.fillStyle = 'white';
            
            const originalFilterEye = eyeMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                eyeMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }

            function drawLandmarkPolygon(landmarkConnections, targetCtx, targetCanvas) {
              if (landmarks && landmarkConnections && landmarkConnections.length > 0) {
                targetCtx.beginPath();
                const firstLandmarkIndex = landmarkConnections[0].start;
                if (landmarks[firstLandmarkIndex]) {
                  const firstPoint = landmarks[firstLandmarkIndex];
                  targetCtx.moveTo(firstPoint.x * targetCanvas.width, firstPoint.y * targetCanvas.height);
                  for (const conn of landmarkConnections) {
                    const endLandmarkIndex = conn.end;
                    if (landmarks[endLandmarkIndex]) {
                      const endPoint = landmarks[endLandmarkIndex];
                      targetCtx.lineTo(endPoint.x * targetCanvas.width, endPoint.y * targetCanvas.height);
                    } else {
                      console.warn(`Landmark index ${endLandmarkIndex} not found.`);
                    }
                  }
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Landmark index ${firstLandmarkIndex} not found for starting point.`);
                }
              }
            }

            function drawCustomLandmarkPolygonFromIndices(vertexIndices, targetCtx, targetCanvas) {
              if (landmarks && vertexIndices && vertexIndices.length > 0) {
                targetCtx.beginPath();
                const firstLandmarkIndex = vertexIndices[0];
                if (landmarks[firstLandmarkIndex]) {
                  const firstPoint = landmarks[firstLandmarkIndex];
                  targetCtx.moveTo(firstPoint.x * targetCanvas.width, firstPoint.y * targetCanvas.height);
                  for (let i = 1; i < vertexIndices.length; i++) {
                    const landmarkIndex = vertexIndices[i];
                    if (landmarks[landmarkIndex]) {
                      const point = landmarks[landmarkIndex];
                      targetCtx.lineTo(point.x * targetCanvas.width, point.y * targetCanvas.height);
                    } else {
                      console.warn(`Landmark index ${landmarkIndex} not found in custom list.`);
                    }
                  }
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Landmark index ${firstLandmarkIndex} not found for starting point in custom list.`);
                }
              }
            }

            function drawTriangleOnMask(vertexIndices, targetCtx, targetCanvas) {
              if (landmarks && vertexIndices && vertexIndices.length === 3) {
                const points = vertexIndices.map(index => landmarks[index]);
                if (points.every(p => p)) {
                  targetCtx.beginPath();
                  targetCtx.moveTo(points[0].x * targetCanvas.width, points[0].y * targetCanvas.height);
                  targetCtx.lineTo(points[1].x * targetCanvas.width, points[1].y * targetCanvas.height);
                  targetCtx.lineTo(points[2].x * targetCanvas.width, points[2].y * targetCanvas.height);
                  targetCtx.closePath();
                  targetCtx.fill();
                } else {
                  console.warn(`Invalid landmark index in triangle: ${vertexIndices}`);
                }
              }
            }

            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYE, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LEFT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_RIGHT_EYEBROW, eyeMaskCtx, eyeMaskCanvas);
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_LIPS, eyeMaskCtx, eyeMaskCanvas);

            const innerMouthTriangles = [
              [78, 95, 88], [78, 178, 87], [95, 178, 96],
              [78, 88, 87], [88, 87, 178], [88, 95, 96],
              [96, 88, 178]
            ];
            innerMouthTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));
            drawTriangleOnMask([185, 292, 61], eyeMaskCtx, eyeMaskCanvas);
            const specificEyebrowTriangles = [
              [107, 55, 65], [63, 46, 52],
              [336, 285, 295], [293, 276, 334]
            ];
            specificEyebrowTriangles.forEach(triangle => drawTriangleOnMask(triangle, eyeMaskCtx, eyeMaskCanvas));

            const additionalEyeVertices = [7,22,23,24,25,26,27,28,29,30,33,56,110,112,130,133,144,145,153,154,155,157,158,159,160,161,163,173,190,243,246,247];
            drawCustomLandmarkPolygonFromIndices(additionalEyeVertices, eyeMaskCtx, eyeMaskCanvas);

            const additionalOtherEyeVertices = [249,252,253,254,255,256,257,258,259,260,263,286,339,341,359,362,373,374,380,381,382,384,385,386,387,388,390,398,414,463,466,467];
            drawCustomLandmarkPolygonFromIndices(additionalOtherEyeVertices, eyeMaskCtx, eyeMaskCanvas);

            eyeMaskCtx.filter = originalFilterEye;

            faceOvalMaskCtx.clearRect(0, 0, faceOvalMaskCanvas.width, faceOvalMaskCanvas.height);
            faceOvalMaskCtx.fillStyle = 'white';
            
            const originalFilterOval = faceOvalMaskCtx.filter;
            if (landmarkMaskBlurRadius > 0) {
                faceOvalMaskCtx.filter = `blur(${landmarkMaskBlurRadius}px)`;
            }
            drawLandmarkPolygon(FaceLandmarker.FACE_LANDMARKS_FACE_OVAL, faceOvalMaskCtx, faceOvalMaskCanvas);
            faceOvalMaskCtx.filter = originalFilterOval;

            effectLayerCtx.clearRect(0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            effectLayerCtx.drawImage(maskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'destination-in';
            effectLayerCtx.drawImage(faceOvalMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'destination-out';
            effectLayerCtx.drawImage(eyeMaskCanvas, 0, 0, effectLayerCanvas.width, effectLayerCanvas.height);
            
            effectLayerCtx.globalCompositeOperation = 'source-over';

            tempBlurLayerCtx.clearRect(0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
            tempBlurLayerCtx.filter = 'none'; // Ensure no CSS/SVG filters are active

            // Draw current video frame to the small blurredVideoCanvas
            blurredVideoCtx.clearRect(0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);
            blurredVideoCtx.drawImage(video, 0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height);

            if (sharpenLevel > 0) {
                const sharpenedImageData = applySharpenEffectJS(blurredVideoCtx, sharpenLevel);
                // Put the sharpened data onto the small intermediate canvas
                sharpenedSmallCtx.putImageData(sharpenedImageData, 0, 0);
                // Draw the small sharpened canvas onto the full-size tempBlurLayerCanvas, scaling it up
                tempBlurLayerCtx.drawImage(sharpenedSmallCanvas, 0, 0, sharpenedSmallCanvas.width, sharpenedSmallCanvas.height, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
                console.log(`[Effect Debug] Applied JS Sharpen. Level: ${sharpenLevel}`);
            } else {
                // If sharpenLevel is 0, draw the original (scaled-down) video frame to tempBlurLayerCanvas, scaling it up
                tempBlurLayerCtx.drawImage(blurredVideoCanvas, 0, 0, blurredVideoCanvas.width, blurredVideoCanvas.height, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
                console.log(`[Effect Debug] Sharpen level is 0, drawing unsharpened scaled video.`);
            }
            
            // Restore original masking and drawing logic
            // Now apply the effect mask (skin mask, excluding eyes/mouth/oval)
            tempBlurLayerCtx.globalCompositeOperation = 'destination-in';
            tempBlurLayerCtx.drawImage(effectLayerCanvas, 0, 0, tempBlurLayerCanvas.width, tempBlurLayerCanvas.height);
            tempBlurLayerCtx.globalCompositeOperation = 'source-over';

            // Draw the tempBlurLayerCanvas (which now contains only the processed skin)
            // onto the main canvas. This will be drawn over the personOnlyCanvas if it was drawn.
            canvasCtx.drawImage(tempBlurLayerCanvas, 0, 0, canvasElement.width, canvasElement.height);

          } else {
            console.warn("[iOS Debug] skinMaskPixelData for smoothing was INVALID or dimensions were zero. Skipping custom effect. This is a strong indicator why effect might not be working on iOS."); 
          }
        } else {
            console.warn("[iOS Debug] No valid segmentation mask OR no landmarks found for this frame. Skipping custom effect."); 
        }
      }
      
      if(segmentationResultsToClose){
        try {
            segmentationResultsToClose.close();
        } catch(e){
            console.warn("Error closing categoryMask:", e);
        }
      }

      if (isPredicting) {
        window.requestAnimationFrame(predictWebcam);
      } else {
        console.log("[predictWebcam END] Not predicting, loop will stop.");
      }
    }
  </script>
</body>
</html>
